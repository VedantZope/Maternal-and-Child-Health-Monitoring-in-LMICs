{"cells":[{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Function Definitions\n","\n","In this section, we define some of the necessary functions required for our analysis.\n","\n","1. **`country_region_mapping(df)`**: This function takes a dataframe and maps each country code in the 'DHSID' column to its corresponding region. It first creates a dictionary mapping from country codes to regions, then extracts the country codes from the 'DHSID' column, corrects any mislabeled country codes, and finally maps the regions and countries to a new column 'target_region'.\n","\n","2. **`split_data_country_wise(df)`**: This function splits the data into training, development, and test sets for each country, by dropping the label columns, and then for each unique country, splits the data into training *(80%)*, development *(10%)*, and test *(10%)* sets. It then concatenates all the training sets, all the development sets, and all the test sets to form the final `X_train`, `X_dev`, `X_test`, `y_train`, `y_dev`, and `y_test`. *This is done to ensure that each set has representation from all the countries.*\n","\n","\n","3. **`one_hot_encode_fit(X_train)`**: This function one-hot encodes the categorical columns of the training data. It takes the training data, identifies the categorical columns, and then applies one-hot encoding to these columns using the `ColumnTransformer` and `OneHotEncoder` classes from `sklearn`. It returns the encoded training data and the transformer.\n","\n","4. **`one_hot_encode_transform(X, transformer)`**: This function applies one-hot encoding to a dataframe using a previously fitted transformer. It takes a dataframe and a transformer, applies the transformer to the dataframe, and returns the encoded dataframe.\n","\n","5. **`obj_columns(df)`**: This function takes a dataframe and returns a list of the column names of the columns with object datatype.\n","\n","6. **`mcrmse(y_true, y_pred)`**: This function computes the mean column-wise root mean square error between the true and predicted labels. It takes the true labels and the predicted labels, computes the square error between them, takes the mean of the square errors for each label, takes the square root of these means, and then takes the mean of all the square roots.\n","\n","7. **`make_predictions(input_df)`**: This function preprocesses the input dataframe and uses an ensemble of trained models to make predictions. It takes the input dataframe, ensures it has been preprocessed, loads the trained models, scores, and weights, makes predictions using each model in the ensemble, computes the ensemble predictions, and then converts the predictions to a dataframe with appropriate column names.\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T02:56:53.869056Z","iopub.status.busy":"2023-08-31T02:56:53.868247Z","iopub.status.idle":"2023-08-31T02:56:55.261973Z","shell.execute_reply":"2023-08-31T02:56:55.260670Z","shell.execute_reply.started":"2023-08-31T02:56:53.869010Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import joblib\n","import gc\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","\n","\n","def country_region_mapping(df):\n","    # Country code to region mapping\n","    country_to_region = {}\n","    country_to_region.update({code: \"East Asia & Pacific\" for code in {\"BD\", \"KH\", \"ID\", \"LA\", \"MM\", \"NP\", \"PH\", \"TL\", \"VN\", \"GU\"}})\n","    country_to_region.update({code: \"Central Asia\" for code in {\"TJ\", \"UZ\", \"KG\", \"KZ\"}})\n","    country_to_region.update({code: \"Europe & Central Asia\" for code in {\"AL\", \"AM\", \"AZ\", \"GE\", \"MD\", \"MK\", \"RS\", \"TR\", \"UA\"}})\n","    country_to_region.update({code: \"Sub-Saharan Africa\" for code in {\"AO\", \"BJ\", \"BF\", \"BU\", \"CM\", \"CF\", \"CI\", \"CD\", \"ET\", \"GA\", \"GH\", \"GN\", \"GY\", \"KE\", \"KM\", \"LS\", \"LB\", \"MD\", \"MW\", \"ML\", \"MZ\", \"NG\", \"NM\", \"RW\", \"SN\", \"SL\", \"SZ\", \"TD\", \"TG\", \"ZA\", \"TZ\", \"UG\", \"ZM\", \"ZW\"}})\n","    country_to_region.update({code: \"North Africa & Middle East\" for code in {\"EG\", \"JO\", \"YE\", \"MA\", \"LB\", \"MB\"}})\n","    country_to_region.update({code: \"South Asia\" for code in {\"AF\", \"BD\", \"IN\", \"PK\", \"NP\", \"IA\"}})\n","    country_to_region.update({code: \"Latin America & Caribbean\" for code in {\"BO\", \"CO\", \"DR\", \"HT\", \"HN\", \"MX\", \"PE\", \"NI\"}})\n","\n","    # Extract country codes from DHSID\n","    df['Country_Code'] = df['DHSID'].str.extract(r'([A-Za-z]+)')[0]\n","    \n","    # Correct any mislabeled country codes\n","    df.loc[df[\"Country_Code\"] == \"DHS\", \"Country_Code\"] = \"BD\"\n","\n","    # Map regions and countries\n","    df['target_region'] = df['Country_Code'].map(country_to_region)\n","\n","    return df\n","\n","def split_data_country_wise(df):\n","    X = df.drop(columns=labels)\n","    y = df[labels]\n","    # Unique country codes\n","    countries = X['Country_Code'].unique()\n","\n","    # Initialize empty lists to store the split data\n","    X_train_list, X_dev_list, X_test_list = [], [], []\n","    y_train_list, y_dev_list, y_test_list = [], [], []\n","\n","    # Split the data for each country\n","    for country in countries:\n","        X_country = X[df['Country_Code'] == country]\n","        y_country = y[df['Country_Code'] == country]\n","        \n","        X_train_country, X_temp, y_train_country, y_temp = train_test_split(X_country, y_country, test_size=0.2, random_state=1)\n","        X_dev_country, X_test_country, y_dev_country, y_test_country = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1)\n","\n","        X_train_list.append(X_train_country)\n","        X_dev_list.append(X_dev_country)\n","        X_test_list.append(X_test_country)\n","\n","        y_train_list.append(y_train_country)\n","        y_dev_list.append(y_dev_country)\n","        y_test_list.append(y_test_country)\n","\n","    # Concatenate the splits\n","    X_train = pd.concat(X_train_list, ignore_index=True)\n","    X_dev = pd.concat(X_dev_list, ignore_index=True)\n","    X_test = pd.concat(X_test_list, ignore_index=True)\n","\n","    y_train = pd.concat(y_train_list, ignore_index=True)\n","    y_dev = pd.concat(y_dev_list, ignore_index=True)\n","    y_test = pd.concat(y_test_list, ignore_index=True)\n","\n","    return X_train,X_dev,X_test,y_train,y_dev,y_test\n","\n","def one_hot_encode_fit(X_train):\n","    # Specify the columns to be encoded\n","    categorical_columns = X_train.select_dtypes(include=['object']).columns.tolist()\n","    # Create transformer\n","    transformer = ColumnTransformer(\n","        transformers=[\n","            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)],\n","        remainder='passthrough')\n","    X_train_encoded = transformer.fit_transform(X_train)\n","    return X_train_encoded, transformer\n","\n","def one_hot_encode_transform(X, transformer):\n","    X_encoded = transformer.transform(X)\n","    return X_encoded\n","\n","def obj_columns(df):\n","    return(df.columns[df.dtypes == 'object'].tolist())\n","\n","def mcrmse(y_true, y_pred):\n","    return np.mean(np.sqrt(np.mean(np.square(y_true - y_pred), axis=0)))\n","\n","def make_predictions(input_df):\n","    # Ensure the input dataframe has been preprocessed\n","    # Load the trained models, scores, and weights\n","    chains = joblib.load('chains.joblib')\n","    weights = joblib.load('weights.joblib')\n","    \n","    # Make predictions using each model in the ensemble\n","    predictions = [chain.predict(input_df) for chain in chains]\n","    \n","    # Compute the ensemble predictions\n","    final_predictions = np.zeros_like(predictions[0])\n","    for weight, prediction in zip(weights, predictions):\n","        final_predictions += weight * prediction\n","\n","    # Convert predictions to a dataframe with appropriate column names\n","    final_predictions_df = pd.DataFrame(final_predictions, columns=labels)\n","        \n","    return final_predictions_df\n"]},{"cell_type":"markdown","metadata":{},"source":["---\n","## Managing Large Datasets\n","The initial challenge in our project was the management of multiple large datasets that exceeded memory limits. To address this, we implemented two key strategies to make data manipulation more efficient.\n","\n","- We converted the dataset from CSV to Parquet format. This significantly improved memory efficiency and enabled faster data access and processing.\n","\n","- We downcasted the variables to the lowest possible datatype without losing precision, reducing the memory footprint of the dataset and facilitating efficient data handling.\n","---\n","## Data Preprocessing\n","In this step, we load the `gee_features` and `training labels` file, drop irrelevant columns, and remove duplicate data points.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"34524991-9461-4be9-b815-cc9b340923b1","_uuid":"41a16a77-c6f6-47d3-b86d-6dd08d73ba57","collapsed":false,"execution":{"iopub.execute_input":"2023-08-31T02:56:58.364789Z","iopub.status.busy":"2023-08-31T02:56:58.363947Z","iopub.status.idle":"2023-08-31T02:57:06.513184Z","shell.execute_reply":"2023-08-31T02:57:06.511994Z","shell.execute_reply.started":"2023-08-31T02:56:58.364726Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["df = pd.read_parquet(\"/kaggle/input/stanford-data/gee_features_10pct.parquet\")\n","df.drop_duplicates(subset = 'DHSID',inplace=True)\n","df.drop(columns = ['key1','DHSCC','DATUM','CCFIPS','SOURCE','new_ind','ADM1FIPS','ADM1FIPSNA','ADM1NAME','ADM1SALBCO','ADM1SALBNA','ADM1SALNA','F21','F22','F23','ZONECO','ZONENA'], inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T01:00:54.151321Z","iopub.status.busy":"2023-08-31T01:00:54.150874Z","iopub.status.idle":"2023-08-31T01:00:54.194956Z","shell.execute_reply":"2023-08-31T01:00:54.193796Z","shell.execute_reply.started":"2023-08-31T01:00:54.151286Z"},"trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T01:00:54.197497Z","iopub.status.busy":"2023-08-31T01:00:54.196376Z","iopub.status.idle":"2023-08-31T01:00:54.544934Z","shell.execute_reply":"2023-08-31T01:00:54.543698Z","shell.execute_reply.started":"2023-08-31T01:00:54.197426Z"},"trusted":true},"outputs":[],"source":["tl = pd.read_csv(\"/kaggle/input/stanford-data/training_label.csv\")\n","tl.drop_duplicates(subset = 'DHSID',inplace=True)\n","tl.drop(columns=['URBAN_RURA','DHSCLUST','DHSYEAR', 'LATNUM', 'LONGNUM'],inplace = True)\n","\n","labels = ['Mean_BMI', 'Median_BMI', 'Unmet_Need_Rate', 'Under5_Mortality_Rate','Skilled_Birth_Attendant_Rate', 'Stunted_Rate']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T01:00:54.548661Z","iopub.status.busy":"2023-08-31T01:00:54.546260Z","iopub.status.idle":"2023-08-31T01:00:54.565833Z","shell.execute_reply":"2023-08-31T01:00:54.564626Z","shell.execute_reply.started":"2023-08-31T01:00:54.548613Z"},"trusted":true},"outputs":[],"source":["tl.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Merging Ground Truth and Features\n","To prepare the data for model training, we need to merge the ground truth (`training labels`) with the `gee_features` based on the common column 'DHSID'. This will ensure that each row in the dataset has both the features and the corresponding label.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T01:00:54.567478Z","iopub.status.busy":"2023-08-31T01:00:54.566912Z","iopub.status.idle":"2023-08-31T01:01:13.307109Z","shell.execute_reply":"2023-08-31T01:01:13.306070Z","shell.execute_reply.started":"2023-08-31T01:00:54.567443Z"},"trusted":true},"outputs":[],"source":["merged_df1 = pd.merge(df, tl, on='DHSID', how='right')\n","merged_df1 = merged_df1.dropna(subset=labels)\n","merged_df1 = merged_df1.dropna(subset = merged_df1[['key3']].columns.to_list())\n","merged_df1.reset_index(inplace=True,drop=True)\n","\n","# del df,tl\n","# gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## Additional Data\n","\n","We have integrated additional data from elite global source, such as the *World Health Organisation*.\n","\n","The data was queried using the following methods:\n","\n","- **World Bank Data**: The data was queried using the `wbdata API` (Python module) by feeding in the country and the year, then the retrieved data was transformed and stored in a dataframe, with `DHSID` used as the key to merge with initial data.\n","-The data was retireved only for unique `Country_Code` - `DHSYEAR` pair, and then appended to the dataframe for all the countries\n","\n","The workflow for retrieving this data has been designed to save progress periodically(written to a `pickle` file), allowing the process to be restarted from the last saved checkpoint if data retrieval is stopped or fails.\n","\n","More information about the workflow and the data can be found here: [GitHub Link](your_github_link_here)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T01:01:13.309135Z","iopub.status.busy":"2023-08-31T01:01:13.308501Z","iopub.status.idle":"2023-08-31T01:01:18.058530Z","shell.execute_reply":"2023-08-31T01:01:18.057469Z","shell.execute_reply.started":"2023-08-31T01:01:13.309099Z"},"trusted":true},"outputs":[],"source":["df_worldbank = pd.read_csv(\"/kaggle/input/stanford-data/WorldBank_Features.csv\")\n","df_worldbank.drop(columns = ['DHSYEAR','Country_Code','SpatialDim','TimeDim'],inplace = True)\n","merged_df = pd.merge(merged_df1, df_worldbank, on='DHSID', how='left')"]},{"cell_type":"markdown","metadata":{},"source":["### Feature Selection\n","Selecting the *optimum number of features*, as well as the *most relevant features*, is crucial for building an efficient model. For this purpose, we train a boosting model (**XGBoost**) using a *multi-target regression wrapper* on all the merged features. We then compute the **feature importance** for each feature. Evaluation is done using *k-fold cross-validation*.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T01:01:18.064266Z","iopub.status.busy":"2023-08-31T01:01:18.063892Z","iopub.status.idle":"2023-08-31T01:15:03.233823Z","shell.execute_reply":"2023-08-31T01:15:03.232153Z","shell.execute_reply.started":"2023-08-31T01:01:18.064237Z"},"trusted":true},"outputs":[],"source":["%%time\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.multioutput import MultiOutputRegressor\n","import xgboost as xgb\n","\n","fea_df = merged_df.drop(columns = labels)\n","target_labels = merged_df[labels]\n","\n","# Encode categorical variables using label encoding\n","label_encoders = {}\n","for column in fea_df.select_dtypes(include='object').columns:\n","    label_encoders[column] = LabelEncoder()\n","    fea_df[column] = label_encoders[column].fit_transform(fea_df[column])\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(fea_df, target_labels, test_size=0.2, random_state=42)\n","\n","\n","# Train the XGBoost model using the MultiOutputRegressor wrapper\n","model = MultiOutputRegressor(xgb.XGBRegressor(objective ='reg:squarederror'))\n","model.fit(X_train, y_train)\n","\n","# Compute the feature importances\n","feature_importances = np.zeros(X_train.shape[1])\n","for estimator in model.estimators_:\n","    feature_importances += estimator.feature_importances_\n","\n","# Normalize the feature importances\n","feature_importances /= np.sum(feature_importances)\n","\n","# Create a DataFrame to hold the feature importances\n","importances_df = pd.DataFrame({\n","    'Feature': X_train.columns,\n","    'Importance': feature_importances\n","})\n","\n","# Sort the DataFrame by the importances\n","importances_df = importances_df.sort_values(by='Importance', ascending=False)\n","importances_df.to_csv(\"Feature_Importance.csv\",index = False)\n","# del fea_df,target_labels,df,X_train, X_test, y_train, y_test\n","# gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## Optimum Number of Features\n","\n","The optimum number of features is then selected based on the graph of **MCRMSE** (Mean Column-wise Root Mean Square Error) score *(k-fold mean)* vs the *number of features*. The code below plots the graph for it. Based on the graph, we can observe that there is an slight elbow at the **number of features = 350**, so we choose it as the **optimum number of features**.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T01:26:21.236705Z","iopub.status.busy":"2023-08-31T01:26:21.235412Z","iopub.status.idle":"2023-08-31T01:50:00.081626Z","shell.execute_reply":"2023-08-31T01:50:00.080479Z","shell.execute_reply.started":"2023-08-31T01:26:21.236648Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import xgboost as xgb\n","from tqdm.notebook import tqdm\n","from sklearn.model_selection import cross_val_score\n","from sklearn.multioutput import MultiOutputRegressor\n","\n","sorted_features = importances_df['Feature'].values\n","\n","X_train, X_dev, X_test, y_train, y_dev, y_test = split_data_country_wise(country_region_mapping(merged_df).drop(columns=['DHSID']))\n","\n","# Define the range of number of features to use\n","# In our actual code, we tried running till no. of features = 1000\n","num_features_range = range(10, 501, 20)\n","\n","# Initialize a list to hold the validation scores\n","validation_scores = []\n","\n","# For each number of features in the range\n","for num_features in tqdm(num_features_range):\n","    # Select the top num_features most important features\n","    selected_features = sorted_features[:num_features]\n","    X_train_selected = X_train[selected_features]\n","    X_dev_selected = X_dev[selected_features]\n","    \n","    X_train_selected, encoder = one_hot_encode_fit(X_train_selected)\n","    X_dev_selected = one_hot_encode_transform(X_dev_selected,encoder)\n","    \n","    # Train the model\n","    model = MultiOutputRegressor(xgb.XGBRegressor(objective='reg:squarederror'))\n","    model.fit(X_train_selected, y_train)\n","    \n","    y_pred = model.predict(X_dev_selected)\n","    \n","    # In the actual code, K fold Validation(folds = 10) was used so that we get a robust idea of evaluation\n","    validation_score = mcrmse(y_dev, y_pred)\n","    validation_scores.append(validation_score)\n","\n","# Plot the validation scores against the number of features\n","# Plot the validation scores against the number of features\n","plt.plot(num_features_range, validation_scores)\n","plt.xlabel('Number of Features')\n","plt.ylabel('Validation Score')\n","plt.title('Validation Score vs Number of Features')\n","plt.grid()\n","plt.show()\n","\n","\n","#del X_train, X_dev, X_test, y_train, y_dev, y_test\n","#gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T02:04:05.347496Z","iopub.status.busy":"2023-08-31T02:04:05.347006Z","iopub.status.idle":"2023-08-31T02:04:07.010060Z","shell.execute_reply":"2023-08-31T02:04:07.008891Z","shell.execute_reply.started":"2023-08-31T02:04:05.347462Z"},"trusted":true},"outputs":[],"source":["X_train, X_dev, X_test, y_train, y_dev, y_test = split_data_country_wise(merged_df.drop(columns=['DHSID']))\n","\n","num_features = 350\n","# In actual run we used 350 features based on the plot, more info about it can be found in the report\n","sorted_features = importances_df['Feature'].values\n","selected_features = sorted_features[:num_features]\n","\n","X_train = X_train[selected_features]\n","X_dev = X_dev[selected_features]\n","X_test = X_test[selected_features]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T02:04:29.777840Z","iopub.status.busy":"2023-08-31T02:04:29.777383Z","iopub.status.idle":"2023-08-31T02:04:29.804025Z","shell.execute_reply":"2023-08-31T02:04:29.803126Z","shell.execute_reply.started":"2023-08-31T02:04:29.777802Z"},"trusted":true},"outputs":[],"source":["X_train, encoder = one_hot_encode_fit(X_train)\n","X_dev = one_hot_encode_transform(X_dev, encoder)\n","X_test = one_hot_encode_transform(X_test, encoder)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T02:04:33.254101Z","iopub.status.busy":"2023-08-31T02:04:33.253207Z","iopub.status.idle":"2023-08-31T02:04:33.259874Z","shell.execute_reply":"2023-08-31T02:04:33.258663Z","shell.execute_reply.started":"2023-08-31T02:04:33.254051Z"},"trusted":true},"outputs":[],"source":["X_train = pd.DataFrame(X_train)\n","X_dev = pd.DataFrame(X_dev)\n","X_test = pd.DataFrame(X_test)\n","\n","X_train_cols = X_train.columns.to_list()"]},{"cell_type":"markdown","metadata":{},"source":["## Imputation of Missing Values\n","\n","In this section, we handle missing values in our dataset. Missing data is a common occurrence in real-world data and needs to be addressed before training a model. We use the K-nearest neighbors (KNN) algorithm to impute missing values in the numerical columns of our dataset.\n","\n","1. Separate out the numerical columns from the dataset.\n","2. Apply KNN imputation on the numerical columns.\n","3. Update the original dataset with the imputed numerical values.\n","4. Impute the remaining missing values in the dataset using the median of each column.\n","\n","The KNN imputer uses the mean of the 'k' most similar instances (or neighbors) to impute missing values. We use **'k=5'** for our imputer(this was decided after comprehensive testing with various k values). If there are still any missing values left after the KNN imputation, we fill them with the median of the corresponding column.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T02:05:11.685879Z","iopub.status.busy":"2023-08-31T02:05:11.685479Z","iopub.status.idle":"2023-08-31T02:05:19.983626Z","shell.execute_reply":"2023-08-31T02:05:19.982546Z","shell.execute_reply.started":"2023-08-31T02:05:11.685846Z"},"trusted":true},"outputs":[],"source":["from sklearn.impute import KNNImputer\n","\n","# Initialize KNNImputer\n","imputer = KNNImputer(n_neighbors=5)\n","\n","# 1. Separate out the numerical columns\n","numerical_cols = X_train.select_dtypes(exclude=['object']).columns\n","\n","X_train_numerical = X_train[numerical_cols].copy()\n","X_dev_numerical = X_dev[numerical_cols].copy()\n","X_test_numerical = X_test[numerical_cols].copy()\n","\n","# 2. Apply KNN imputation on numerical columns\n","X_train_numerical_imputed = imputer.fit_transform(X_train_numerical)\n","X_dev_numerical_imputed = imputer.transform(X_dev_numerical)\n","X_test_numerical_imputed = imputer.transform(X_test_numerical)\n","\n","# Convert imputed data back to dataframe\n","X_train_numerical_df = pd.DataFrame(X_train_numerical_imputed, columns=numerical_cols, index=X_train.index)\n","X_dev_numerical_df = pd.DataFrame(X_dev_numerical_imputed, columns=numerical_cols, index=X_dev.index)\n","X_test_numerical_df = pd.DataFrame(X_test_numerical_imputed, columns=numerical_cols, index=X_test.index)\n","\n","# 3. Update the original dataset with the imputed numerical values\n","X_train.update(X_train_numerical_df)\n","X_dev.update(X_dev_numerical_df)\n","X_test.update(X_test_numerical_df)\n","\n","#imputating the remaining values with the median of the data\n","X_train = X_train.fillna(X_train.median())\n","X_dev = X_dev.fillna(X_train.median())\n","X_test = X_test.fillna(X_train.median())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T02:05:47.827780Z","iopub.status.busy":"2023-08-31T02:05:47.827079Z","iopub.status.idle":"2023-08-31T02:05:47.859696Z","shell.execute_reply":"2023-08-31T02:05:47.858591Z","shell.execute_reply.started":"2023-08-31T02:05:47.827734Z"},"trusted":true},"outputs":[],"source":["X_train.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Optimum Chaining Sequence using Greedy Algorithm\n","\n","8. **`evaluate_order(model, order, X_train, y_train, X_dev, y_dev)`**: This function evaluates the mean column-wise root mean square error (MCRMSE) of a model with a given order of the labels. It takes the model, the current order of the labels, the training and development sets, completes the ordering of the labels, fits a `RegressorChain` with the given order on the training data, predicts the development data, and computes the MCRMSE of the predictions.\n","\n","9. **`full_greedy_search(model, X_train, y_train, X_dev, y_dev)`**: This function finds the best order of the labels for a given model using a greedy algorithm. It takes the model and the training and development sets, initializes the current order as empty and the set of all targets as all the labels, and iteratively adds the label that results in the lowest MCRMSE to the current order, until all labels have been added to the order. It returns the best order found and the history of orders and scores.\n","\n","These two functions are used in finding the best order for the regression chain. They use a greedy algorithm to find the next label to be added to the order."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T07:00:47.826739Z","iopub.status.busy":"2023-08-31T07:00:47.826330Z","iopub.status.idle":"2023-08-31T07:00:50.314329Z","shell.execute_reply":"2023-08-31T07:00:50.313179Z","shell.execute_reply.started":"2023-08-31T07:00:47.826708Z"},"trusted":true},"outputs":[],"source":["%%time\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","from catboost import CatBoostRegressor\n","from sklearn.multioutput import RegressorChain\n","\n","def evaluate_order(model, order, X_train, y_train, X_dev, y_dev):\n","    # Complete the ordering\n","    full_order = order + [i for i in range(y_train.shape[1]) if i not in order]\n","\n","    chain = RegressorChain(model, order=full_order)\n","    chain.fit(X_train, y_train)\n","    y_pred = chain.predict(X_dev)\n","    score = mcrmse(y_dev, y_pred)\n","\n","    return score\n","\n","def full_greedy_search(model, X_train, y_train, X_dev, y_dev):\n","    all_targets = list(range(y_train.shape[1]))\n","    current_order = []\n","    history = []\n","    \n","    while all_targets:\n","        best_score = float('inf')\n","        best_target = None\n","\n","        for t in all_targets:\n","            temp_order = current_order + [t]\n","            score = evaluate_order(model, temp_order, X_train, y_train, X_dev, y_dev)\n","            \n","            # Capture history for insights on progression\n","            history.append((temp_order, score))\n","            \n","            if score < best_score:\n","                best_score = score\n","                best_target = t\n","\n","        print(f\"Added target {best_target} to the order. Current best score: {best_score}\")\n","        \n","        current_order.append(best_target)\n","        all_targets.remove(best_target)\n","\n","    return current_order, history\n","\n","lgb_model = LGBMRegressor()\n","xgb_model = XGBRegressor()\n","cat_model = CatBoostRegressor(verbose=0)\n","\n","print(75*\"-\")\n","xgb_best_order, search_history = full_greedy_search(xgb_model, X_train, y_train, X_dev, y_dev)\n","print(f\"Best order found for xgb: {xgb_best_order}\")\n","print(75*\"-\")\n","lgb_best_order, search_history = full_greedy_search(lgb_model, X_train, y_train, X_dev, y_dev)\n","print(f\"Best order found for lgbm: {lgb_best_order}\")\n","print(75*\"-\")\n","cat_best_order, search_history = full_greedy_search(cat_model, X_train, y_train, X_dev, y_dev)\n","print(f\"Best order found for cat: {cat_best_order}\")\n","print(75*\"-\")"]},{"cell_type":"markdown","metadata":{},"source":["## Hyperparameter Optimization using Optuna\n","\n","In this section, we use Optuna for hyperparameter tuning. Optuna is an open-source hyperparameter optimization framework in Python. It is designed to optimize machine learning model's hyperparameters with a user-friendly interface, which is key to the efficient development of machine learning models.\n","\n","The `objective` function defined below is what Optuna will optimize. It takes an Optuna `trial` object and:\n","\n","1. Suggests a model type (XGBoost, LightGBM, or CatBoost) and the corresponding hyperparameters for the model.\n","2. Fits a `RegressorChain` with the suggested model and hyperparameters and the best order of the labels found in the previous section on the training data.\n","3. Predicts the development data and computes the MCRMSE of the predictions.\n","\n","Optuna will then try to find the hyperparameters that minimize the MCRMSE.\n","\n","We use a `MedianPruner` to stop the optimization of trials that do not appear promising, and run the optimization for 50 trials. In the actual run, we conducted the optimization with 200 trials.\n","\n","After the optimization, we print the best parameters and score for each model type.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","import optuna\n","def objective(trial):\n","    try:\n","        # 1. Model selection\n","        model_type = trial.suggest_categorical(\"model_type\", [\"xgb\", \"lgb\", \"cat\"])\n","\n","        # 2. Hyperparameter definitions based on the model\n","        if model_type == \"xgb\":\n","            model = XGBRegressor(\n","                random_state=1,\n","                learning_rate=trial.suggest_float(\"xgb_learning_rate\", 0.01, 0.3, log=True),\n","                n_estimators=trial.suggest_int(\"xgb_n_estimators\", 50, 300),\n","                max_depth=trial.suggest_int(\"xgb_max_depth\", 2, 10),\n","                min_child_weight=trial.suggest_int(\"xgb_min_child_weight\", 1, 10),\n","                subsample=trial.suggest_float(\"xgb_subsample\", 0.5, 1),\n","                colsample_bytree=trial.suggest_float(\"xgb_colsample_bytree\", 0.5, 1),\n","                reg_alpha=trial.suggest_float(\"xgb_reg_alpha\", 1e-3, 10.0, log=True),\n","                reg_lambda=trial.suggest_float(\"xgb_reg_lambda\", 1e-3, 10.0, log=True),\n","#                 tree_method=\"gpu_hist\"  # Enable GPU training\n","            )\n","            chain = RegressorChain(model, order=xgb_best_order)\n","            chain.fit(X_train, y_train)\n","\n","        elif model_type == \"lgb\":\n","            model = LGBMRegressor(\n","                random_state=1,\n","                learning_rate=trial.suggest_float(\"lgb_learning_rate\", 0.01, 0.3, log=True),\n","                n_estimators=trial.suggest_int(\"lgb_n_estimators\", 50, 300),\n","                max_depth=trial.suggest_int(\"lgb_max_depth\", 2, 10),\n","                num_leaves=trial.suggest_int(\"lgb_num_leaves\", 2, 2**trial.suggest_int(\"lgb_max_depth\", 2, 10)),\n","                min_child_samples=trial.suggest_int(\"lgb_min_child_samples\", 5, 100),\n","                subsample=trial.suggest_float(\"lgb_subsample\", 0.5, 1),\n","                colsample_bytree=trial.suggest_float(\"lgb_colsample_bytree\", 0.5, 1),\n","                reg_alpha=trial.suggest_float(\"lgb_reg_alpha\", 1e-3, 10.0, log=True),\n","                reg_lambda=trial.suggest_float(\"lgb_reg_lambda\", 1e-3, 10.0, log=True),\n","#                 device=\"gpu\"  # Enable GPU acceleration\n","            )\n","            chain = RegressorChain(model, order=lgb_best_order)\n","            chain.fit(X_train, y_train)\n","            \n","        else:\n","            model = CatBoostRegressor(\n","                random_seed=1,\n","                learning_rate=trial.suggest_float(\"cat_learning_rate\", 0.01, 0.3, log=True),\n","                n_estimators=trial.suggest_int(\"cat_n_estimators\", 50, 300),\n","                depth=trial.suggest_int(\"cat_depth\", 2, 10),\n","                l2_leaf_reg=trial.suggest_float(\"cat_l2_leaf_reg\", 1e-3, 10.0, log=True),\n","                border_count=trial.suggest_int(\"cat_border_count\", 5, 200),\n","                subsample=trial.suggest_float(\"cat_subsample\", 0.5, 1),\n","                verbose=0,\n","#                 task_type='GPU'  # Enable GPU training\n","            )\n","        chain = RegressorChain(model, order=cat_best_order)\n","        chain.fit(X_train, y_train)\n","        \n","        y_pred = chain.predict(X_dev)\n","        return mcrmse(y_dev, y_pred)\n","    \n","    except Exception as e:\n","        print(f\"Error in trial {trial.number}: {e}\")\n","        return None\n","\n","pruner = optuna.pruners.MedianPruner()\n","study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n","study.optimize(objective, n_trials=50)\n","# Actual iteration was run with n_trials = 200\n","\n","best_trials = {}\n","for trial in study.trials:\n","    if trial.state == optuna.trial.TrialState.COMPLETE:\n","        model_type = trial.params['model_type']\n","        if model_type not in best_trials or trial.value < best_trials[model_type].value:\n","            best_trials[model_type] = trial\n","\n","best_params_dict = {}\n","print('-' * 75)\n","for model_type, trial in best_trials.items():\n","    print(f\"Best parameters for {model_type}: {trial.params}\")\n","    print(f\"Score: {trial.value}\")\n","    print('-' * 75)\n","    best_params_dict[model_type] = trial.params"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lgb_params = best_trials['lgb'].params\n","xgb_params = best_trials['xgb'].params\n","cat_params = best_trials['cat'].params\n","cat_params['verbose'] = 0\n","\n","# Remove the 'model_type' key from the parameters dictionaries\n","lgb_params.pop('model_type')\n","xgb_params.pop('model_type')\n","cat_params.pop('model_type')\n","\n","# Remove model-specific prefixes from the parameter names\n","lgb_params = {key.replace('lgb_', ''): value for key, value in lgb_params.items()}\n","xgb_params = {key.replace('xgb_', ''): value for key, value in xgb_params.items()}\n","cat_params = {key.replace('cat_', ''): value for key, value in cat_params.items()}\n","\n","# # Current best hyperparameters for the entire data, that were used for training the final model\n","# # XGBRegressor hyperparameters\n","# xgb_params = {\n","#     'learning_rate': 0.01257586059952504,\n","#     'n_estimators': 963,\n","#     'max_depth': 9,\n","#     'min_child_weight': 8,\n","#     'colsample_bytree': 0.4062908772431949,\n","#     'subsample': 0.9087746601113265,\n","#     'random_state': 1,\n","# #    'tree_method': 'gpu_hist'\n","# }\n","\n","# # LGBMRegressor hyperparameters\n","# lgb_params = {\n","#     'learning_rate': 0.012929977832547674,\n","#     'n_estimators': 978,\n","#     'max_depth': 9,\n","#     'num_leaves': 232,\n","#     'feature_fraction': 0.38509329814179993,\n","#     'bagging_fraction': 0.9382159152916805,\n","#     'bagging_freq': 1,\n","#     'random_state': 1,\n","# #    'device': 'gpu'\n","# }\n","\n","# # CatBoostRegressor hyperparameters\n","# cat_params = {\n","#     'learning_rate': 0.06452361591392032,\n","#     'iterations': 996,\n","#     'depth': 9,\n","#     'l2_leaf_reg': 8.376118948612561,\n","#     'verbose': 0,\n","#     'random_state': 1,\n","# #    'task_type': 'GPU'\n","# }\n","\n","lgb_model = LGBMRegressor(**lgb_params)\n","xgb_model = XGBRegressor(**xgb_params)\n","cat_model = CatBoostRegressor(**cat_params)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Training and Evaluation\n","\n","In this section, we train a `RegressorChain` with each of the three models (`XGBoost`, `LightGBM`, and `CatBoost`) and the optimized order of the labels and the best params found in the previous sections. We then compute the MCRMSE on the development set for each model and use Bayesian Model Averaging (BMA) for calculating the final prediction. These weights are used to compute the ensemble predictions on the development set and the test set.\n","\n","1. Define the optimized order for the `RegressorChain` and the models with the optimized hyperparameters.\n","2. Train a `RegressorChain` with each model and calculate the MCRMSE on the development set.\n","3. Calculate the BMA weights based on the inverse of the MCRMSE scores.\n","4. Save the trained models, MCRMSE scores, and BMA weights for later use.\n","5. Compute the ensemble predictions on the development set and the test set using the BMA weights and compute the MCRMSE of the ensemble predictions.\n","\n","Finally, we evaluate the performance of the ensemble on the test set.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-08-31T01:15:06.468302Z","iopub.status.idle":"2023-08-31T01:15:06.468701Z","shell.execute_reply":"2023-08-31T01:15:06.468513Z","shell.execute_reply.started":"2023-08-31T01:15:06.468495Z"},"trusted":true},"outputs":[],"source":["%%time\n","# Define optimised order for the RegressorChain based on the previous code\n","orders = [xgb_best_order, lgb_best_order, cat_best_order] \n","# Define the models based on the optimised hyperparameters\n","models = [xgb_model, lgb_model, cat_model]\n","\n","# Train a RegressorChain with each model and calculate validation scores\n","chains = []\n","scores = []\n","for i, (model, order) in enumerate(zip(models, orders)):\n","    print(f\"Training model {i+1}/{len(models)}\")\n","    chain = RegressorChain(model, order=order, random_state=1)\n","    chain.fit(X_train,y_train)\n","    chains.append(chain)\n","    y_pred_dev = chain.predict(X_dev)\n","    score = mcrmse(y_dev, y_pred_dev)\n","    scores.append(score)\n","    print(f\"Validation MCRMSE for model {i+1}: {score}\")\n","\n","# Calculate BMA weights based on the inverse of the validation scores\n","weights = [1/score for score in scores]\n","weights = [weight/sum(weights) for weight in weights]  # normalize so that weights sum to 1\n","\n","# Save models, scores, and weights for later use\n","joblib.dump(chains, 'chains.joblib')\n","joblib.dump(scores, 'scores.joblib')\n","joblib.dump(weights, 'weights.joblib')\n","\n","# Now, compute the ensemble predictions on the dev set and the MCRMSE\n","predictions_dev = [chain.predict(X_dev) for chain in chains]\n","final_predictions_dev = np.zeros_like(predictions_dev[0])\n","for weight, prediction in zip(weights, predictions_dev):\n","    final_predictions_dev += weight * prediction\n","\n","final_score_dev = mcrmse(y_dev, final_predictions_dev)\n","print(f\"Final ensemble validation MCRMSE: {final_score_dev}\")\n","\n","# Finally, evaluate on the test set\n","predictions_test = [chain.predict(X_test) for chain in chains]\n","final_predictions_test = np.zeros_like(predictions_test[0])\n","for weight, prediction in zip(weights, predictions_test):\n","    final_predictions_test += weight * prediction\n","\n","final_score_test = mcrmse(y_test, final_predictions_test)\n","print(f\"Final ensemble test MCRMSE: {final_score_test}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Making Predictions\n","\n","Now that we have trained our model and evaluated its performance, we can use it to make predictions on new data. In this section, we will load the trained models, and BMA weights saved in the previous section and use them to compute the ensemble predictions on new data.\n","\n","1. Load the Submission Feature Data and preprocess the new data in the same way as the training data.\n","2. Load the trained models, and BMA weights from the previous section.\n","3. Compute the ensemble predictions on the new data using the BMA weights.\n","4. Save the predictions to a file for further analysis or submission."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T03:04:14.568514Z","iopub.status.busy":"2023-08-31T03:04:14.568114Z","iopub.status.idle":"2023-08-31T03:04:14.863231Z","shell.execute_reply":"2023-08-31T03:04:14.862000Z","shell.execute_reply.started":"2023-08-31T03:04:14.568480Z"},"trusted":true},"outputs":[],"source":["sample = pd.read_csv(\"/kaggle/input/stanford-data/sample submission.csv\")\n","sf = pd.merge(df, sample, on='DHSID', how='inner')\n","sf.reset_index(inplace=True, drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T03:05:55.884703Z","iopub.status.busy":"2023-08-31T03:05:55.884258Z","iopub.status.idle":"2023-08-31T03:05:55.913165Z","shell.execute_reply":"2023-08-31T03:05:55.911933Z","shell.execute_reply.started":"2023-08-31T03:05:55.884672Z"},"trusted":true},"outputs":[],"source":["sf.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T03:04:27.596543Z","iopub.status.busy":"2023-08-31T03:04:27.595915Z","iopub.status.idle":"2023-08-31T03:04:28.523424Z","shell.execute_reply":"2023-08-31T03:04:28.522235Z","shell.execute_reply.started":"2023-08-31T03:04:27.596510Z"},"trusted":true},"outputs":[],"source":["sf_worldbank = pd.read_csv(\"/kaggle/input/stanford-data/WorldBank_Sample.csv\")\n","sf_worldbank.drop(columns = ['DHSYEAR','Country_Code','SpatialDim','TimeDim'],inplace = True)\n","merged_sf = pd.merge(sf, sf_worldbank, on='DHSID', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T03:09:14.728556Z","iopub.status.busy":"2023-08-31T03:09:14.728146Z","iopub.status.idle":"2023-08-31T03:09:14.766147Z","shell.execute_reply":"2023-08-31T03:09:14.764836Z","shell.execute_reply.started":"2023-08-31T03:09:14.728523Z"},"trusted":true},"outputs":[],"source":["merged_sf.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-08-31T01:15:06.487996Z","iopub.status.idle":"2023-08-31T01:15:06.488592Z","shell.execute_reply":"2023-08-31T01:15:06.488406Z","shell.execute_reply.started":"2023-08-31T01:15:06.488380Z"},"trusted":true},"outputs":[],"source":["merged_sf = merged_sf[selected_features]\n","merged_sf = one_hot_encode_transform(merged_sf, encoder)\n","merged_sf = pd.DataFrame(merged_sf)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-08-31T01:15:06.497167Z","iopub.status.idle":"2023-08-31T01:15:06.497579Z","shell.execute_reply":"2023-08-31T01:15:06.497407Z","shell.execute_reply.started":"2023-08-31T01:15:06.497367Z"},"trusted":true},"outputs":[],"source":["# Performing KNN Imputation tranformation for this data\n","merged_sf_numerical = merged_sf[numerical_cols].copy()\n","merged_sf_numerical_imputed = imputer.transform(merged_sf_numerical)\n","\n","# Convert imputed data back to dataframe\n","merged_sf_numerical_df = pd.DataFrame(merged_sf_numerical_imputed, columns=numerical_cols, index=merged_sf.index)\n","\n","# 3. Update the original sample dataset with the imputed numerical values\n","merged_sf.update(merged_sf_numerical_df)\n","merged_sf = merged_sf.fillna(X_train.median())\n","# merged_sf = merged_sf[X_train_cols]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-08-31T01:15:06.503689Z","iopub.status.idle":"2023-08-31T01:15:06.504056Z","shell.execute_reply":"2023-08-31T01:15:06.503897Z","shell.execute_reply.started":"2023-08-31T01:15:06.503881Z"},"trusted":true},"outputs":[],"source":["predictions = make_predictions(merged_sf)\n","predictions = pd.concat([sf['DHSID'].reset_index(),predictions],axis=1).drop(['index'],axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["## Predictions\n","Below are the first few rows of the predictions:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-08-31T01:15:06.504944Z","iopub.status.idle":"2023-08-31T01:15:06.505415Z","shell.execute_reply":"2023-08-31T01:15:06.505202Z","shell.execute_reply.started":"2023-08-31T01:15:06.505180Z"},"trusted":true},"outputs":[],"source":["#finally saving the final predictions as a csv\n","predictions.to_csv('Final_Submission.csv',index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
