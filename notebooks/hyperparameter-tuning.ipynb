{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0da76299",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-08-13T06:55:58.189155Z",
     "iopub.status.busy": "2023-08-13T06:55:58.188598Z",
     "iopub.status.idle": "2023-08-13T06:55:58.213894Z",
     "shell.execute_reply": "2023-08-13T06:55:58.212499Z"
    },
    "papermill": {
     "duration": 0.036466,
     "end_time": "2023-08-13T06:55:58.217781",
     "exception": false,
     "start_time": "2023-08-13T06:55:58.181315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/imputed-scaled-lgbm-15/X_dev_scaled_imputed.csv\n",
      "/kaggle/input/imputed-scaled-lgbm-15/y_dev_scaled.csv\n",
      "/kaggle/input/imputed-scaled-lgbm-15/X_test_scaled_imputed.csv\n",
      "/kaggle/input/imputed-scaled-lgbm-15/X_train_scaled_imputed.csv\n",
      "/kaggle/input/imputed-scaled-lgbm-15/y_test_scaled.csv\n",
      "/kaggle/input/imputed-scaled-lgbm-15/y_train_scaled.csv\n",
      "/kaggle/input/hyperparam-helper/y_train.parquet\n",
      "/kaggle/input/hyperparam-helper/X_train.parquet\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ec587a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T06:55:58.231016Z",
     "iopub.status.busy": "2023-08-13T06:55:58.230442Z",
     "iopub.status.idle": "2023-08-13T06:56:02.495849Z",
     "shell.execute_reply": "2023-08-13T06:56:02.494833Z"
    },
    "papermill": {
     "duration": 4.275517,
     "end_time": "2023-08-13T06:56:02.498599",
     "exception": false,
     "start_time": "2023-08-13T06:55:58.223082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "\n",
    "\n",
    "def evaluate_order_kfold(models, order, X, y, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "\n",
    "        fold_scores = []\n",
    "        for model in models:\n",
    "            chain = RegressorChain(model, order=order, random_state=1)\n",
    "            chain.fit(X_train_fold, y_train_fold)\n",
    "            y_pred_val = chain.predict(X_val_fold)\n",
    "            score = mcrmse(y_val_fold, y_pred_val)\n",
    "            fold_scores.append(score)\n",
    "\n",
    "        scores.append(np.mean(fold_scores))\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "def greedy_search_order(models, X, y, n_splits=5):\n",
    "    all_targets = list(range(y.shape[1]))\n",
    "    current_order = []\n",
    "    while len(all_targets) > 0:\n",
    "        best_score = float('inf')\n",
    "        best_target = None\n",
    "\n",
    "        for target in all_targets:\n",
    "            temp_order = current_order + [target]\n",
    "            temp_score = evaluate_order_kfold(models, temp_order, X, y, n_splits=n_splits)\n",
    "\n",
    "            if temp_score < best_score:\n",
    "                best_score = temp_score\n",
    "                best_target = target\n",
    "\n",
    "        current_order.append(best_target)\n",
    "        all_targets.remove(best_target)\n",
    "        \n",
    "    return current_order\n",
    "\n",
    "\n",
    "def make_predictions(input_df):\n",
    "    \"\"\"\n",
    "    This function will preprocess the input dataframe, and use the ensemble of trained models to make predictions.\n",
    "    Args:\n",
    "    - input_df: The input dataframe. It should have the same features as your training data.\n",
    "    \n",
    "    Returns:\n",
    "    - final_predictions_df: Predictions for the input dataframe in the form of a dataframe with appropriate column names.\n",
    "    \"\"\"\n",
    "    # Ensure the input dataframe has been preprocessed\n",
    "    # Example: if you used scaling before, ensure `input_df` has also been scaled\n",
    "    \n",
    "    # Load the trained models, scores, and weights\n",
    "    chains = joblib.load('chains.joblib')\n",
    "    weights = joblib.load('weights.joblib')\n",
    "    \n",
    "    # Make predictions using each model in the ensemble\n",
    "    predictions = [chain.predict(input_df) for chain in chains]\n",
    "    \n",
    "    # Compute the ensemble predictions\n",
    "    final_predictions = np.zeros_like(predictions[0])\n",
    "    for weight, prediction in zip(weights, predictions):\n",
    "        final_predictions += weight * prediction\n",
    "\n",
    "    # Convert predictions to a dataframe with appropriate column names\n",
    "    final_predictions_df = pd.DataFrame(final_predictions, columns=labels_list())\n",
    "        \n",
    "    return final_predictions_df\n",
    "\n",
    "# def apply_robust_scaling(df):\n",
    "#     df = df.drop(columns=obj_columns(df))\n",
    "#     scaler = RobustScaler()\n",
    "#     df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "#     return df_scaled\n",
    "\n",
    "def drop_high_nan(df, threshold=0.85):\n",
    "    \"\"\"\n",
    "    Drops columns from the dataframe that have a proportion of NaN values greater than the specified threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Input DataFrame.\n",
    "    - threshold: Proportion threshold for NaN values (default is 0.85).\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with columns having NaN proportion greater than the threshold dropped.\n",
    "    \"\"\"\n",
    "    nan_proportion = df.isnull().mean()\n",
    "    columns_to_drop = nan_proportion[nan_proportion > threshold].index.tolist()\n",
    "    return df.drop(columns=columns_to_drop)\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, df):\n",
    "        numerical_data = df.select_dtypes(include=['float64', 'int64']).values\n",
    "        self.scaler.fit(numerical_data)\n",
    "\n",
    "    def transform(self, df):\n",
    "        df_transformed = df.copy()\n",
    "        numerical_features = df_transformed.select_dtypes(include=['float64', 'int64']).columns\n",
    "        df_transformed[numerical_features] = self.scaler.transform(df_transformed[numerical_features].values)\n",
    "        return df_transformed\n",
    "\n",
    "    def inverse_transform(self, df):\n",
    "        df_original = df.copy()\n",
    "        numerical_features = df_original.select_dtypes(include=['float64', 'int64']).columns\n",
    "        df_original[numerical_features] = self.scaler.inverse_transform(df_original[numerical_features].values)\n",
    "        return df_original\n",
    "\n",
    "class TargetPreprocessor:\n",
    "    def __init__(self, column_names=None):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.column_names = column_names\n",
    "\n",
    "    def fit(self, y):\n",
    "        self.scaler.fit(y)\n",
    "\n",
    "    def transform(self, y):\n",
    "        y_array = self.scaler.transform(y)\n",
    "        if isinstance(y, pd.DataFrame):\n",
    "            return pd.DataFrame(y_array, columns=y.columns, index=y.index)\n",
    "        else:\n",
    "            return y_array\n",
    "\n",
    "    def inverse_transform(self, y_scaled):\n",
    "        y_array = self.scaler.inverse_transform(y_scaled)\n",
    "        if self.column_names:\n",
    "            return pd.DataFrame(y_array, columns=self.column_names)\n",
    "        elif isinstance(y_scaled, pd.DataFrame):\n",
    "            return pd.DataFrame(y_array, columns=y_scaled.columns, index=y_scaled.index)\n",
    "        else:\n",
    "            return y_array\n",
    "\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from lightgbm import LGBMRegressor\n",
    "import pandas as pd\n",
    "\n",
    "class IterativeDataImputer:\n",
    "    def __init__(self, max_iter=10, random_state=None, verbose=0):\n",
    "        lgb_fast_hyperparams = {\n",
    "            'learning_rate': 0.05,\n",
    "            'n_estimators': 300,\n",
    "            'max_depth': 6,\n",
    "            'num_leaves': 64,\n",
    "            'feature_fraction': 0.5,\n",
    "            'bagging_fraction': 0.7,\n",
    "            'bagging_freq': 1,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        self.lgbm_regressor = LGBMRegressor(**lgb_fast_hyperparams)\n",
    "        self.imputer = IterativeImputer(estimator=self.lgbm_regressor, max_iter=max_iter, random_state=random_state, verbose=verbose)\n",
    "\n",
    "    def fit(self, df):\n",
    "        # Select only numerical columns\n",
    "        self.numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "        self.imputer.fit(df[self.numerical_cols])\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df_copy = df.copy()\n",
    "        df_copy[self.numerical_cols] = self.imputer.transform(df[self.numerical_cols])\n",
    "        return df_copy\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        self.fit(df)\n",
    "        return self.transform(df)\n",
    "\n",
    "\n",
    "\n",
    "# def apply_knn_iterative_imputation(df, knn_n_neighbors, max_iter, random_state=None):\n",
    "#     warnings.warn(\"it is a good idea to apply scaling before imputation,ignore if already applied, if not apply_robust_scaling() and apply_standard_scaling() functions can be used\")\n",
    "#     \"\"\"\n",
    "#     Apply KNN imputation for columns with less than 30% missing data and Iterative imputation for columns with more.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - df: The input dataframe with missing values.\n",
    "#     - knn_n_neighbors: Number of neighboring samples to use for KNN imputation.\n",
    "#     - max_iter: Maximum number of imputation iterations for IterativeImputer.\n",
    "#     - random_state: Seed used by the random number generator for IterativeImputer.\n",
    "    \n",
    "#     Returns:\n",
    "#     - df: DataFrame with imputed values.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Identify columns based on missing data threshold\n",
    "#     missing_data = df.isna().mean()\n",
    "#     cols_lt_30 = missing_data[missing_data < 0.3].index.tolist()\n",
    "#     cols_gt_30 = missing_data[missing_data >= 0.3].index.tolist()\n",
    "\n",
    "#     # KNN imputation for columns with < 30% missing data\n",
    "#     if len(cols_lt_30) > 0:\n",
    "#         print(\"KNN imputer for columns with < 30% missing data started\")\n",
    "#         knn_imputer = KNNImputer(n_neighbors=knn_n_neighbors)\n",
    "#         df[cols_lt_30] = knn_imputer.fit_transform(df[cols_lt_30])\n",
    "    \n",
    "#     # Iterative imputation for columns with >= 30% missing data\n",
    "#     if len(cols_gt_30) > 0:\n",
    "#         print(\"Iterative imputer for columns with >= 30% missing data started\")\n",
    "#         iter_imputer = IterativeImputer(estimator=DecisionTreeRegressor(), max_iter=max_iter, random_state=random_state)\n",
    "#         df[cols_gt_30] = iter_imputer.fit_transform(df[cols_gt_30])\n",
    "\n",
    "#     return df\n",
    "\n",
    "def load_csv_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df\n",
    "\n",
    "def load_parquet_data(filepath):\n",
    "    df = pd.read_parquet(filepath)\n",
    "    return df\n",
    "\n",
    "def labels_list():\n",
    "    labels = ['Mean_BMI', 'Median_BMI',\n",
    "           'Unmet_Need_Rate', 'Under5_Mortality_Rate',\n",
    "         'Skilled_Birth_Attendant_Rate', 'Stunted_Rate']\n",
    "    return labels\n",
    "\n",
    "def heatmap(df):    \n",
    "    plt.figure(figsize=(40, 30)) # Increase the size of the figure\n",
    "    sns.heatmap(df.isnull(), cbar=False, cmap='binary')\n",
    "    plt.show()\n",
    "\n",
    "def one_hot_encode(df, columns):\n",
    "    df = pd.get_dummies(df, columns=columns)\n",
    "    return df\n",
    "\n",
    "def country_region_mapping(df):\n",
    "    # Country code to region mapping\n",
    "    country_to_region = {}\n",
    "    country_to_region.update({code: \"East Asia & Pacific\" for code in {\"BD\", \"KH\", \"ID\", \"LA\", \"MM\", \"NP\", \"PH\", \"TL\", \"VN\", \"GU\"}})\n",
    "    country_to_region.update({code: \"Central Asia\" for code in {\"TJ\", \"UZ\", \"KG\", \"KZ\"}})\n",
    "    country_to_region.update({code: \"Europe & Central Asia\" for code in {\"AL\", \"AM\", \"AZ\", \"GE\", \"MD\", \"MK\", \"RS\", \"TR\", \"UA\"}})\n",
    "    country_to_region.update({code: \"Sub-Saharan Africa\" for code in {\"AO\", \"BJ\", \"BF\", \"BU\", \"CM\", \"CF\", \"CI\", \"CD\", \"ET\", \"GA\", \"GH\", \"GN\", \"GY\", \"KE\", \"KM\", \"LS\", \"LB\", \"MD\", \"MW\", \"ML\", \"MZ\", \"NG\", \"NM\", \"RW\", \"SN\", \"SL\", \"SZ\", \"TD\", \"TG\", \"ZA\", \"TZ\", \"UG\", \"ZM\", \"ZW\"}})\n",
    "    country_to_region.update({code: \"North Africa & Middle East\" for code in {\"EG\", \"JO\", \"YE\", \"MA\", \"LB\", \"MB\"}})\n",
    "    country_to_region.update({code: \"South Asia\" for code in {\"AF\", \"BD\", \"IN\", \"PK\", \"NP\", \"IA\"}})\n",
    "    country_to_region.update({code: \"Latin America & Caribbean\" for code in {\"BO\", \"CO\", \"DR\", \"HT\", \"HN\", \"MX\", \"PE\", \"NI\"}})\n",
    "\n",
    "    # Extract country codes from DHSID\n",
    "    df['Country_Code'] = df['DHSID'].str.extract(r'([A-Za-z]+)')[0]\n",
    "    \n",
    "    # Correct any mislabeled country codes\n",
    "    df.loc[df[\"Country_Code\"] == \"DHS\", \"Country_Code\"] = \"BD\"\n",
    "\n",
    "    # Map regions and countries\n",
    "    df['target_region'] = df['Country_Code'].map(country_to_region)\n",
    "    df['target_country'] = df['Country_Code'].factorize()[0]\n",
    "\n",
    "    return df\n",
    "\n",
    "#df1 is main dataframe and df2 is secondary dataframe, merged on basis of DHSID\n",
    "def merge_dataframes(df, df2):\n",
    "    return df.merge(df2, on='DHSID', how='inner')\n",
    "\n",
    "def obj_columns(df):\n",
    "    return(df.columns[df.dtypes == 'object'].tolist())\n",
    "\n",
    "def split_data_country_wise(df):\n",
    "    if 'Country_Code' not in df.columns.to_list():\n",
    "        raise Exception(\"dataframe doesnt have a column named Country_Code, use country_region_mapping(df) to get it\")\n",
    "  \n",
    "    X = df.drop(columns=labels)\n",
    "    y = df[labels]\n",
    "    # Unique country codes\n",
    "    countries = df['Country_Code'].unique()\n",
    "\n",
    "    # Initialize empty lists to store the split data\n",
    "    X_train_list, X_dev_list, X_test_list = [], [], []\n",
    "    y_train_list, y_dev_list, y_test_list = [], [], []\n",
    "\n",
    "    # Split the data for each country\n",
    "    for country in countries:\n",
    "        X_country = X[df['Country_Code'] == country]\n",
    "        y_country = y[df['Country_Code'] == country]\n",
    "        \n",
    "        X_train_country, X_temp, y_train_country, y_temp = train_test_split(X_country, y_country, test_size=0.2, random_state=1)\n",
    "        X_dev_country, X_test_country, y_dev_country, y_test_country = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1)\n",
    "\n",
    "        X_train_list.append(X_train_country)\n",
    "        X_dev_list.append(X_dev_country)\n",
    "        X_test_list.append(X_test_country)\n",
    "\n",
    "        y_train_list.append(y_train_country)\n",
    "        y_dev_list.append(y_dev_country)\n",
    "        y_test_list.append(y_test_country)\n",
    "\n",
    "    # Concatenate the splits\n",
    "    X_train = pd.concat(X_train_list, ignore_index=True)\n",
    "    X_dev = pd.concat(X_dev_list, ignore_index=True)\n",
    "    X_test = pd.concat(X_test_list, ignore_index=True)\n",
    "\n",
    "    y_train = pd.concat(y_train_list, ignore_index=True)\n",
    "    y_dev = pd.concat(y_dev_list, ignore_index=True)\n",
    "    y_test = pd.concat(y_test_list, ignore_index=True)\n",
    "\n",
    "    return X_train,X_dev,X_test,y_train,y_dev,y_test\n",
    "\n",
    "def add_temporal_features(df):\n",
    "    \"\"\"\n",
    "    Adds temporal features to the dataframe.\n",
    "    \n",
    "    For countries with data across multiple years:\n",
    "    1. Computes the year-on-year difference for each feature.\n",
    "    2. Generates aggregated temporal features like mean, median, \n",
    "       and standard deviation for each feature for each country over the years.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with original data.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with additional temporal features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Excluding labels and other non-feature columns\n",
    "    exclude_cols = obj_columns(df) + labels_list()\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Sort the dataframe by 'Country_Code' and 'DHSYEAR' for proper computation\n",
    "    df = df.sort_values(by=['Country_Code', 'DHSYEAR'])\n",
    "    \n",
    "    new_cols = []\n",
    "    \n",
    "    # 1. Compute year-on-year difference for each feature\n",
    "    for col in feature_cols:\n",
    "        new_cols.append(pd.Series(df.groupby('Country_Code')[col].diff(), name=f\"{col}_yearly_diff\"))\n",
    "    \n",
    "    # 2. Generate aggregated temporal features for each feature for each country\n",
    "    for col in feature_cols:\n",
    "        new_cols.append(pd.Series(df.groupby('Country_Code')[col].transform('mean'), name=f\"{col}_mean\"))\n",
    "        new_cols.append(pd.Series(df.groupby('Country_Code')[col].transform('median'), name=f\"{col}_median\"))\n",
    "        new_cols.append(pd.Series(df.groupby('Country_Code')[col].transform('std'), name=f\"{col}_std\"))\n",
    "    \n",
    "    # Concatenate original DataFrame with new columns\n",
    "    df = pd.concat([df] + new_cols, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def nan_percentage(df):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with columns and their corresponding % of NaN values in descending order.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with columns: 'Column Name' and 'NaN %'\n",
    "    \"\"\"\n",
    "    # Calculate the percentage of NaNs for each column\n",
    "    nan_percent = df.isnull().mean() * 100\n",
    "\n",
    "    # Create a DataFrame with results\n",
    "    result_df = pd.DataFrame({\n",
    "        'Column Name': nan_percent.index,\n",
    "        'NaN %': nan_percent.values\n",
    "    })\n",
    "\n",
    "    # Sort the DataFrame in descending order\n",
    "    result_df = result_df.sort_values(by='NaN %', ascending=False)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Define the function to calculate MCRMSE\n",
    "def mcrmse(y_true, y_pred):\n",
    "    return np.mean(np.sqrt(np.mean(np.square(y_true - y_pred), axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf324eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T06:56:02.510425Z",
     "iopub.status.busy": "2023-08-13T06:56:02.509187Z",
     "iopub.status.idle": "2023-08-13T06:56:35.912161Z",
     "shell.execute_reply": "2023-08-13T06:56:35.910874Z"
    },
    "papermill": {
     "duration": 33.412291,
     "end_time": "2023-08-13T06:56:35.915420",
     "exception": false,
     "start_time": "2023-08-13T06:56:02.503129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_scaled_imputed = load_csv_data(\"/kaggle/input/imputed-scaled-lgbm-15/X_train_scaled_imputed.csv\")\n",
    "X_dev_scaled_imputed = load_csv_data(\"/kaggle/input/imputed-scaled-lgbm-15/X_dev_scaled_imputed.csv\")\n",
    "X_test_scaled_imputed = load_csv_data(\"/kaggle/input/imputed-scaled-lgbm-15/X_test_scaled_imputed.csv\")\n",
    "\n",
    "y_train_scaled = load_csv_data(\"/kaggle/input/imputed-scaled-lgbm-15/y_train_scaled.csv\")\n",
    "y_dev_scaled = load_csv_data(\"/kaggle/input/imputed-scaled-lgbm-15/y_dev_scaled.csv\")\n",
    "y_test_scaled = load_csv_data(\"/kaggle/input/imputed-scaled-lgbm-15/y_test_scaled.csv\")\n",
    "\n",
    "# Compute the medians from the training data\n",
    "medians = X_train_scaled_imputed.select_dtypes(include=['float64', 'int64']).median()\n",
    "\n",
    "# Impute NaN values in all datasets with the computed medians\n",
    "X_train_scaled_imputed.fillna(medians, inplace=True)\n",
    "X_dev_scaled_imputed.fillna(medians, inplace=True)\n",
    "X_test_scaled_imputed.fillna(medians, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28d08b5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T06:56:35.927049Z",
     "iopub.status.busy": "2023-08-13T06:56:35.926588Z",
     "iopub.status.idle": "2023-08-13T06:56:36.837968Z",
     "shell.execute_reply": "2023-08-13T06:56:36.836866Z"
    },
    "papermill": {
     "duration": 0.92081,
     "end_time": "2023-08-13T06:56:36.841062",
     "exception": false,
     "start_time": "2023-08-13T06:56:35.920252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = load_parquet_data(\"/kaggle/input/hyperparam-helper/X_train.parquet\")\n",
    "y_train = load_parquet_data(\"/kaggle/input/hyperparam-helper/y_train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "222434cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T06:56:36.853202Z",
     "iopub.status.busy": "2023-08-13T06:56:36.852345Z",
     "iopub.status.idle": "2023-08-13T06:56:39.183773Z",
     "shell.execute_reply": "2023-08-13T06:56:39.182417Z"
    },
    "papermill": {
     "duration": 2.341196,
     "end_time": "2023-08-13T06:56:39.187179",
     "exception": false,
     "start_time": "2023-08-13T06:56:36.845983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_preprocessor = DataPreprocessor()\n",
    "\n",
    "# Fit the preprocessor on the training data\n",
    "features_preprocessor.fit(X_train)\n",
    "\n",
    "# Create an instance of the preprocessor for targets\n",
    "target_preprocessor = TargetPreprocessor()\n",
    "\n",
    "# Fit the preprocessor on the training target data\n",
    "target_preprocessor.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7acf6a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T06:56:39.200625Z",
     "iopub.status.busy": "2023-08-13T06:56:39.198384Z",
     "iopub.status.idle": "2023-08-13T06:56:39.209730Z",
     "shell.execute_reply": "2023-08-13T06:56:39.208784Z"
    },
    "papermill": {
     "duration": 0.020629,
     "end_time": "2023-08-13T06:56:39.212502",
     "exception": false,
     "start_time": "2023-08-13T06:56:39.191873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.multioutput import RegressorChain\n",
    "# from xgboost import XGBRegressor\n",
    "# from lightgbm import LGBMRegressor\n",
    "# from catboost import CatBoostRegressor\n",
    "# import optuna\n",
    "# from sklearn.model_selection import KFold\n",
    "# optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "# def objective(trial):\n",
    "#     model_type = trial.suggest_categorical(\"model_type\", [\"xgb\", \"lgb\", \"cat\"])\n",
    "#     order = [0, 1, 5, 4, 3, 2]\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "#     scores = []\n",
    "\n",
    "#     for fold_num, (train_idx, valid_idx) in enumerate(kf.split(X_train_scaled_imputed)):\n",
    "#         X_train_fold = X_train_scaled_imputed.iloc[train_idx]\n",
    "#         y_train_fold = y_train_scaled.iloc[train_idx]\n",
    "#         X_valid_fold = X_train_scaled_imputed.iloc[valid_idx]\n",
    "#         y_valid_fold = y_train_scaled.iloc[valid_idx]\n",
    "\n",
    "#         if model_type == \"xgb\":\n",
    "#             model = XGBRegressor(\n",
    "#                 random_state=1,\n",
    "#                 objective='reg:squarederror',\n",
    "#                 learning_rate=trial.suggest_float(\"xgb_learning_rate\", 0.01, 0.3, log=True),\n",
    "#                 n_estimators=trial.suggest_int(\"xgb_n_estimators\", 50, 300),\n",
    "#                 max_depth=trial.suggest_int(\"xgb_max_depth\", 2, 10),\n",
    "#                 min_child_weight=trial.suggest_int(\"xgb_min_child_weight\", 1, 10),\n",
    "#                 subsample=trial.suggest_float(\"xgb_subsample\", 0.5, 1),\n",
    "#                 colsample_bytree=trial.suggest_float(\"xgb_colsample_bytree\", 0.5, 1),\n",
    "#                 reg_alpha=trial.suggest_float(\"xgb_reg_alpha\", 1e-3, 10.0, log=True),\n",
    "#                 reg_lambda=trial.suggest_float(\"xgb_reg_lambda\", 1e-3, 10.0, log=True),\n",
    "#                 verbosity=0\n",
    "#             )\n",
    "\n",
    "#         elif model_type == \"lgb\":\n",
    "#             model = LGBMRegressor(\n",
    "#                 random_state=1,\n",
    "#                 learning_rate=trial.suggest_float(\"lgb_learning_rate\", 0.01, 0.3, log=True),\n",
    "#                 n_estimators=trial.suggest_int(\"lgb_n_estimators\", 50, 300),\n",
    "#                 max_depth=trial.suggest_int(\"lgb_max_depth\", 2, 10),\n",
    "#                 num_leaves=trial.suggest_int(\"lgb_num_leaves\", 2, 2**trial.suggest_int(\"lgb_max_depth\", 2, 10)),\n",
    "#                 min_child_samples=trial.suggest_int(\"lgb_min_child_samples\", 5, 100),\n",
    "#                 subsample=trial.suggest_float(\"lgb_subsample\", 0.5, 1),\n",
    "#                 colsample_bytree=trial.suggest_float(\"lgb_colsample_bytree\", 0.5, 1),\n",
    "#                 reg_alpha=trial.suggest_float(\"lgb_reg_alpha\", 1e-3, 10.0, log=True),\n",
    "#                 reg_lambda=trial.suggest_float(\"lgb_reg_lambda\", 1e-3, 10.0, log=True),\n",
    "#                 verbose=-1\n",
    "#             )\n",
    "\n",
    "#         else:\n",
    "#             model = CatBoostRegressor(\n",
    "#                 random_seed=1,\n",
    "#                 learning_rate=trial.suggest_float(\"cat_learning_rate\", 0.01, 0.3, log=True),\n",
    "#                 n_estimators=trial.suggest_int(\"cat_n_estimators\", 50, 300),\n",
    "#                 depth=trial.suggest_int(\"cat_depth\", 2, 10),\n",
    "#                 l2_leaf_reg=trial.suggest_float(\"cat_l2_leaf_reg\", 1e-3, 10.0, log=True),\n",
    "#                 border_count=trial.suggest_int(\"cat_border_count\", 5, 200),\n",
    "#                 subsample=trial.suggest_float(\"cat_subsample\", 0.5, 1),\n",
    "#                 loss_function='RMSE',\n",
    "#                 od_type='Iter',\n",
    "#                 od_wait=10,\n",
    "#                 verbose=0\n",
    "#             )\n",
    "        \n",
    "#         chain = RegressorChain(model, order=order)\n",
    "#         chain.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "#         y_pred_fold = chain.predict(X_valid_fold)\n",
    "#         y_valid_fold_unscaled = target_preprocessor.inverse_transform(y_valid_fold)\n",
    "#         y_pred_original = target_preprocessor.inverse_transform(y_pred_fold)\n",
    "        \n",
    "#         fold_score = mcrmse(y_valid_fold, y_pred_original)\n",
    "#         scores.append(fold_score)\n",
    "        \n",
    "#         print(f\"Model {model_type}, Fold {fold_num + 1} completed with score: {fold_score}\")\n",
    "\n",
    "#     mean_score = np.mean(scores)\n",
    "#     print(f\"Finished iteration with model_type: {model_type}. Mean score: {mean_score}\")\n",
    "#     return mean_score\n",
    "\n",
    "# pruner = optuna.pruners.MedianPruner()\n",
    "# study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
    "# study.optimize(objective, n_trials=120, n_jobs=-1)\n",
    "\n",
    "# best_trials = {}\n",
    "# for trial in study.trials:\n",
    "#     if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "#         model_type = trial.params['model_type']\n",
    "#         if model_type not in best_trials or trial.value < best_trials[model_type].value:\n",
    "#             best_trials[model_type] = trial\n",
    "\n",
    "# best_params_dict = {}\n",
    "# for model_type, trial in best_trials.items():\n",
    "#     print(f\"Best parameters for {model_type}: {trial.params}\")\n",
    "#     print(f\"Score: {trial.value}\")\n",
    "#     print('-' * 50)\n",
    "#     best_params_dict[model_type] = trial.params\n",
    "\n",
    "# np.save('best_hyperparams.npy', best_params_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f7dd5fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T06:56:39.224214Z",
     "iopub.status.busy": "2023-08-13T06:56:39.223453Z"
    },
    "papermill": {
     "duration": 1039.211299,
     "end_time": "2023-08-13T07:13:58.428445",
     "exception": false,
     "start_time": "2023-08-13T06:56:39.217146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-13 06:56:40,224] A new study created in memory with name: no-name-a0bf01d2-c03e-42fb-8d14-d2be7ccf98d5\n",
      "[I 2023-08-13 07:00:48,418] Trial 0 finished with value: 10.144405145710238 and parameters: {'model_type': 'lgb', 'lgb_learning_rate': 0.011071359353951064, 'lgb_n_estimators': 173, 'lgb_max_depth': 2, 'lgb_num_leaves': 4, 'lgb_min_child_samples': 76, 'lgb_subsample': 0.8111577482332275, 'lgb_colsample_bytree': 0.8588335935711795, 'lgb_reg_alpha': 0.001058033324136389, 'lgb_reg_lambda': 0.00965811744483018}. Best is trial 0 with value: 10.144405145710238.\n",
      "[I 2023-08-13 07:03:28,003] Trial 2 finished with value: 9.073500382219398 and parameters: {'model_type': 'cat', 'cat_learning_rate': 0.1460804564320867, 'cat_n_estimators': 131, 'cat_depth': 2, 'cat_l2_leaf_reg': 2.658839115618, 'cat_border_count': 8, 'cat_subsample': 0.9665934789076048}. Best is trial 2 with value: 9.073500382219398.\n",
      "[I 2023-08-13 07:06:03,361] Trial 5 finished with value: 10.602011659352149 and parameters: {'model_type': 'lgb', 'lgb_learning_rate': 0.018472274389594343, 'lgb_n_estimators': 52, 'lgb_max_depth': 4, 'lgb_num_leaves': 10, 'lgb_min_child_samples': 67, 'lgb_subsample': 0.9687974947154868, 'lgb_colsample_bytree': 0.6987582615187892, 'lgb_reg_alpha': 0.15955617610016, 'lgb_reg_lambda': 0.1076899625290637}. Best is trial 2 with value: 9.073500382219398.\n",
      "[I 2023-08-13 07:07:11,257] Trial 1 finished with value: 9.622585123012911 and parameters: {'model_type': 'cat', 'cat_learning_rate': 0.021290984213660852, 'cat_n_estimators': 120, 'cat_depth': 5, 'cat_l2_leaf_reg': 6.9804450613890054, 'cat_border_count': 77, 'cat_subsample': 0.9507097341560058}. Best is trial 2 with value: 9.073500382219398.\n",
      "[I 2023-08-13 07:10:26,963] Trial 6 finished with value: 10.04701083536933 and parameters: {'model_type': 'lgb', 'lgb_learning_rate': 0.011586882075615717, 'lgb_n_estimators': 169, 'lgb_max_depth': 2, 'lgb_num_leaves': 4, 'lgb_min_child_samples': 48, 'lgb_subsample': 0.8209811378326809, 'lgb_colsample_bytree': 0.6100580982667642, 'lgb_reg_alpha': 0.06835757892344413, 'lgb_reg_lambda': 0.6669175960957774}. Best is trial 2 with value: 9.073500382219398.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.multioutput import RegressorChain\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "def objective(trial):\n",
    "    try:\n",
    "        # 1. Model selection\n",
    "        model_type = trial.suggest_categorical(\"model_type\", [\"xgb\", \"lgb\", \"cat\"])\n",
    "    \n",
    "        order = [0, 1, 5, 4, 3, 2]\n",
    "        \n",
    "        # 2. Hyperparameter definitions based on the model\n",
    "        if model_type == \"xgb\":\n",
    "            model = XGBRegressor(\n",
    "                random_state=1,\n",
    "                learning_rate=trial.suggest_float(\"xgb_learning_rate\", 0.01, 0.3, log=True),\n",
    "                n_estimators=trial.suggest_int(\"xgb_n_estimators\", 50, 300),\n",
    "                max_depth=trial.suggest_int(\"xgb_max_depth\", 2, 10),\n",
    "                min_child_weight=trial.suggest_int(\"xgb_min_child_weight\", 1, 10),\n",
    "                subsample=trial.suggest_float(\"xgb_subsample\", 0.5, 1),\n",
    "                colsample_bytree=trial.suggest_float(\"xgb_colsample_bytree\", 0.5, 1),\n",
    "                reg_alpha=trial.suggest_float(\"xgb_reg_alpha\", 1e-3, 10.0, log=True),\n",
    "                reg_lambda=trial.suggest_float(\"xgb_reg_lambda\", 1e-3, 10.0, log=True)\n",
    "            )\n",
    "        elif model_type == \"lgb\":\n",
    "            model = LGBMRegressor(\n",
    "                random_state=1,\n",
    "                learning_rate=trial.suggest_float(\"lgb_learning_rate\", 0.01, 0.3, log=True),\n",
    "                n_estimators=trial.suggest_int(\"lgb_n_estimators\", 50, 300),\n",
    "                max_depth=trial.suggest_int(\"lgb_max_depth\", 2, 10),\n",
    "                num_leaves=trial.suggest_int(\"lgb_num_leaves\", 2, 2**trial.suggest_int(\"lgb_max_depth\", 2, 10)),\n",
    "                min_child_samples=trial.suggest_int(\"lgb_min_child_samples\", 5, 100),\n",
    "                subsample=trial.suggest_float(\"lgb_subsample\", 0.5, 1),\n",
    "                colsample_bytree=trial.suggest_float(\"lgb_colsample_bytree\", 0.5, 1),\n",
    "                reg_alpha=trial.suggest_float(\"lgb_reg_alpha\", 1e-3, 10.0, log=True),\n",
    "                reg_lambda=trial.suggest_float(\"lgb_reg_lambda\", 1e-3, 10.0, log=True)\n",
    "            )\n",
    "        else:\n",
    "            model = CatBoostRegressor(\n",
    "                random_seed=1,\n",
    "                learning_rate=trial.suggest_float(\"cat_learning_rate\", 0.01, 0.3, log=True),\n",
    "                n_estimators=trial.suggest_int(\"cat_n_estimators\", 50, 300),\n",
    "                depth=trial.suggest_int(\"cat_depth\", 2, 10),\n",
    "                l2_leaf_reg=trial.suggest_float(\"cat_l2_leaf_reg\", 1e-3, 10.0, log=True),\n",
    "                border_count=trial.suggest_int(\"cat_border_count\", 5, 200),\n",
    "                subsample=trial.suggest_float(\"cat_subsample\", 0.5, 1),\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "        # Model training\n",
    "        chain = RegressorChain(model, order=order)\n",
    "        chain.fit(X_train_scaled_imputed, y_train_scaled)\n",
    "\n",
    "        y_pred_scaled = chain.predict(X_dev_scaled_imputed)\n",
    "        y_pred_original = target_preprocessor.inverse_transform(y_pred_scaled)\n",
    "        y_dev_original = target_preprocessor.inverse_transform(y_dev_scaled)\n",
    "        return mcrmse(y_dev_original, y_pred_original)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in trial {trial.number}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Hyperparameter optimization\n",
    "pruner = optuna.pruners.MedianPruner()\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
    "study.optimize(objective, n_trials=120, n_jobs=-1)\n",
    "\n",
    "best_params_dict = {}\n",
    "for model_type, trial in best_trials.items():\n",
    "    print(f\"Best parameters for {model_type}: {trial.params}\")\n",
    "    print(f\"Score: {trial.value}\")\n",
    "    print('-' * 50)\n",
    "    best_params_dict[model_type] = trial.params\n",
    "\n",
    "np.save('best_hyperparams.npy', best_params_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1093.208115,
   "end_time": "2023-08-13T07:13:58.440678",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-13T06:55:45.232563",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
